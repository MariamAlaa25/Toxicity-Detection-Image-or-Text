# ğŸ›¡ï¸ Toxicity Detection System (Text & Image)

A Streamlit-based application that detects whether input text or image uploaded are **Toxic** or **Non-Toxic** using a fine-tuned DistilBERT model with LoRA and BLIP for image captioning.

---

## ğŸ“Œ Project Overview

This system allows users to:

* Enter **text** for toxicity classification
* Upload an **image**, generate a caption using BLIP
* Classify the generated caption as **Toxic / Non-Toxic**
* Store prediction history in a CSV file
* Display prediction history inside the Streamlit interface

---

## ğŸ§  Models Used

* **Toxicity Classifier:** DistilBERT + LoRA (PEFT)
* **Image Captioning Model:** BLIP (Salesforce BLIP Image Captioning Base)
* **Framework:** PyTorch
* **Frontend:** Streamlit

---

## âš™ï¸ Installation

Install dependencies:

```bash
pip install torch transformers peft streamlit pillow pandas scikit-learn tqdm
```

## ğŸš€ Run the Application

```bash
streamlit run app.py
```

---

## ğŸ–¼ï¸ Application Features

### 1ï¸âƒ£ Text Toxicity Detection

* User enters text
* Model predicts Toxic or Non-Toxic
* Confidence score is displayed

### 2ï¸âƒ£ Image Toxicity Detection

* User uploads image
* BLIP generates caption
* Caption is classified
* Result + confidence displayed

### 3ï¸âƒ£ Prediction History

* All inputs and outputs saved in `toxicity_history.csv`
* History displayed as a table inside Streamlit

---


## ğŸ“Š Prediction History

| Type  | Text Input      | Generated Caption                          | Prediction | Confidence | Time                |
|-------|----------------|--------------------------------------------|------------|------------|---------------------|
| Image | â€”              | a fire is seen from the side of a building| Toxic      | 72.46%     | 2026-02-17 16:33:22 |
| Text  | i will kill you| â€”                                          | Toxic      | 88.55%     | 2026-02-17 16:33:51 |
| Image | â€”              | a fire is seen from the side of a building| Toxic      | 72.46%     | 2026-02-18 15:19:31 |
| Text  | murder         | â€”                                          | Toxic      | 91.20%     | 2026-02-18 15:20:08 |
| Text  | hate           | â€”                                          | Non-Toxic  | 55.48%     | 2026-02-18 15:32:50 |


---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py
â”œâ”€â”€ imagecaption.py
â”œâ”€â”€ toxicity_history.csv
â”œâ”€â”€ saved_model/
â”œâ”€â”€ text.py
â”œâ”€â”€ Toxic_data_cleaned (2)
â””â”€â”€ README.md
```

---

## ğŸ—„ï¸ Data Storage

Prediction results are stored in:

```
toxicity_history.csv
```

Stored fields:

* Input Type
* Original Text
* Generated Caption
* Prediction
* Confidence
* Timestamp
* Encoded Image (Base64)

---

## ğŸ“Œ Technologies Used

* Python 3.x
* PyTorch
* HuggingFace Transformers
* PEFT (LoRA)
* Streamlit
* Pandas
* BLIP (Image Captioning)

---

## ğŸ“· Screenshots

![Home Page](1.png)

![Text Classification](2.png)

![Image Upload](3.png)

![Generated Caption](4.png)

![Prediction History](5.png)





