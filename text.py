# -*- coding: utf-8 -*-
"""Untitled57.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KKxYh3iLRaCGjm4Ph-Ghwyz9WV7MObyR
"""
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load your CSV
df = pd.read_csv("Toxic_data_cleaned (2).csv")
toxic_classes = {
    'Child Sexual Exploitation',
    'Non-Violent Crimes',
    'Sex-Related Crimes',
    'Suicide & Self-Harm',
    'Violent Crimes',
    'unsafe'
}
texts = df["query"].tolist()
labels = df["Toxic Category"].tolist()

labels = [
    "Toxic" if l in toxic_classes else "Non-Toxic"
    for l in labels
]
# Encode labels as integers
le = LabelEncoder()
labels = le.fit_transform(labels)

# Split: 80% train, 10% val, 10% test
X_train, X_temp, y_train, y_temp = train_test_split(
    texts, labels, test_size=0.2, stratify=labels, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)
#-----------------------------------------------------------------------------
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

max_len = 100

train_encodings = tokenizer(
    X_train, padding=True, truncation=True, max_length=max_len
)
val_encodings = tokenizer(
    X_val, padding=True, truncation=True, max_length=max_len
)
test_encodings = tokenizer(
    X_test, padding=True, truncation=True, max_length=max_len
)
#defines a custom PyTorch Dataset that converts tokenized text inputs and their labels into tensors 
# so they can be efficiently loaded in batches for model training or evaluation.
import torch
from torch.utils.data import Dataset , DataLoader

class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TextDataset(train_encodings, y_train)
val_dataset = TextDataset(val_encodings, y_val)
test_dataset = TextDataset(test_encodings, y_test)

#----------------------------------------------------

batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

from transformers import AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model

num_labels = len(le.classes_)

# Load DistilBERT for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)
# LoRA configuration
lora_config = LoraConfig(
    r=8,                   # low-rank dimension
    lora_alpha=16,          # scaling factor
    target_modules=["q_lin", "v_lin"],  # only Q and V matrices
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS"
)

# Wrap model with LoRA
model = get_peft_model(model, lora_config)
#------------------------------------------------------
# Loading the optimizer(updates all trainable parameters of model using a learning rate of 3 × 10⁻⁴ during training.)
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=3e-4)
#----------------------------------------------------------
#model training 
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
epochs = 10

for epoch in range(epochs):
    # Training
    model.train()
    train_loss = 0
    for batch in tqdm(train_loader):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    print(f"Epoch {epoch+1} train loss: {train_loss/len(train_loader):.4f}")

    # Validation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            predictions = torch.argmax(outputs.logits, dim=-1)
            correct += (predictions == batch["labels"]).sum().item()
            total += batch["labels"].size(0)
    print(f"Epoch {epoch+1} val accuracy: {correct/total:.4f}")
#evaluates the model on the test set
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch in test_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        predictions = torch.argmax(outputs.logits, dim=-1)
        correct += (predictions == batch["labels"]).sum().item()
        total += batch["labels"].size(0)

print(f"Test Accuracy: {correct/total:.4f}")

# Save trained LoRA adapter
model.save_pretrained("saved_model")

# Save tokenizer 
tokenizer.save_pretrained("saved_model")
